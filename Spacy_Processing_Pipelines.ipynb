{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Spacy_Processing_Pipelines.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "xb8Q3w-rYd65",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "614a11fa-a3b3-457c-a631-ebe34a997707"
      },
      "source": [
        "import spacy\n",
        "\n",
        "texts = [\n",
        "    \"Net income was $9.4 million compared to the prior year of $2.7 million.\",\n",
        "    \"Revenue exceeded twelve billion dollars, with a loss of $1b.\",\n",
        "]\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "for doc in nlp.pipe(texts, disable=[\"tagger\", \"parser\"]):\n",
        "    # Do something with the doc here\n",
        "    print([(ent.text, ent.label_) for ent in doc.ents])"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('$9.4 million', 'MONEY'), ('the prior year', 'DATE'), ('$2.7 million', 'MONEY')]\n",
            "[('twelve billion dollars', 'MONEY'), ('1b', 'MONEY')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_jc7DVw5ZMpX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "2d20772e-d63e-431b-bbf6-487eb1e8c69a"
      },
      "source": [
        "import spacy\n",
        "\n",
        "def my_component(doc):\n",
        "    print(\"After tokenization, this doc has {} tokens.\".format(len(doc)))\n",
        "    print(\"The part-of-speech tags are:\", [token.pos_ for token in doc])\n",
        "    if len(doc) < 10:\n",
        "        print(\"This is a pretty short document.\")\n",
        "    return doc\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "nlp.add_pipe(my_component, name=\"print_info\", last=True)\n",
        "print(nlp.pipe_names)  # ['tagger', 'parser', 'ner', 'print_info']\n",
        "doc = nlp(\"This is a sentence.\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['tagger', 'parser', 'ner', 'print_info']\n",
            "After tokenization, this doc has 5 tokens.\n",
            "The part-of-speech tags are: ['DET', 'AUX', 'DET', 'NOUN', 'PUNCT']\n",
            "This is a pretty short document.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EBnXc5pAaLCY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "c44bd08a-00b2-4db7-9376-e7f23bfa2c66"
      },
      "source": [
        "import spacy\n",
        "from spacy.matcher import PhraseMatcher\n",
        "from spacy.tokens import Span\n",
        "\n",
        "class EntityMatcher(object):\n",
        "    name = \"entity_matcher\"\n",
        "\n",
        "    def __init__(self, nlp, terms, label):\n",
        "        patterns = [nlp.make_doc(text) for text in terms]\n",
        "        self.matcher = PhraseMatcher(nlp.vocab)\n",
        "        self.matcher.add(label, None, *patterns)\n",
        "\n",
        "    def __call__(self, doc):\n",
        "        matches = self.matcher(doc)\n",
        "        for match_id, start, end in matches:\n",
        "            span = Span(doc, start, end, label=match_id)\n",
        "            doc.ents = list(doc.ents) + [span]\n",
        "        return doc\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "terms = (\"cat\", \"dog\", \"tree kangaroo\", \"giant sea spider\")\n",
        "entity_matcher = EntityMatcher(nlp, terms, \"ANIMAL\")\n",
        "\n",
        "nlp.add_pipe(entity_matcher, after=\"ner\")\n",
        "\n",
        "print(nlp.pipe_names)  # The components in the pipeline\n",
        "\n",
        "doc = nlp(\"This is a text about Barack Obama and a tree kangaroo\")\n",
        "print([(ent.text, ent.label_) for ent in doc.ents])"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['tagger', 'parser', 'ner', 'entity_matcher']\n",
            "[('Barack Obama', 'LOC'), ('tree kangaroo', 'ANIMAL')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9-F_QDo4ammw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "297040db-9cb1-4d2d-f12a-a5a20c35f257"
      },
      "source": [
        "#custom sentence segmentation\n",
        "import spacy\n",
        "\n",
        "def custom_sentencizer(doc):\n",
        "    for i, token in enumerate(doc[:-2]):\n",
        "        # Define sentence start if pipe + titlecase token\n",
        "        if token.text == \"|\" and doc[i+1].is_title:\n",
        "            doc[i+1].is_sent_start = True\n",
        "        else:\n",
        "            # Explicitly set sentence start to False otherwise, to tell\n",
        "            # the parser to leave those tokens alone\n",
        "            doc[i+1].is_sent_start = False\n",
        "    return doc\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "nlp.add_pipe(custom_sentencizer, before=\"parser\")  # Insert before the parser\n",
        "doc = nlp(\"This is. A sentence. | This is. Another sentence.\")\n",
        "for sent in doc.sents:\n",
        "    print(sent.text)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "This is. A sentence. |\n",
            "This is. Another sentence.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6O2R3VqcBt9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from spacy.tokens import Doc, Span, Token\n",
        "\n",
        "fruits = [\"apple\", \"pear\", \"banana\", \"orange\", \"strawberry\"]\n",
        "is_fruit_getter = lambda token: token.text in fruits\n",
        "has_fruit_getter = lambda obj: any([t.text in fruits for t in obj])\n",
        "\n",
        "Token.set_extension(\"is_fruit\", getter=is_fruit_getter)\n",
        "Doc.set_extension(\"has_fruit\", getter=has_fruit_getter)\n",
        "Span.set_extension(\"has_fruit\", getter=has_fruit_getter)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GPTAQd9ScG-0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "doc = nlp(\"I have an apple and a melon\")\n",
        "assert doc[3]._.is_fruit   # get Token attributes\n",
        "assert not doc[0]._.is_fruit\n",
        "assert doc._.has_fruit        # get Doc attributes\n",
        "assert doc[1:4]._.has_fruit   # get Span attributes"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}