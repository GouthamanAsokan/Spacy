{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Spacy_Training_Model_2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "flJlYma8fl9H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Updating the parser\n",
        "from __future__ import unicode_literals, print_function\n",
        "\n",
        "import plac\n",
        "import random\n",
        "from pathlib import Path\n",
        "import spacy\n",
        "from spacy.util import minibatch, compounding\n",
        "\n",
        "\n",
        "# training data\n",
        "TRAIN_DATA = [\n",
        "    (\n",
        "        \"They trade mortgage-backed securities.\",\n",
        "        {\n",
        "            \"heads\": [1, 1, 4, 4, 5, 1, 1],\n",
        "            \"deps\": [\"nsubj\", \"ROOT\", \"compound\", \"punct\", \"nmod\", \"dobj\", \"punct\"],\n",
        "        },\n",
        "    ),\n",
        "    (\n",
        "        \"I like London and Berlin.\",\n",
        "        {\n",
        "            \"heads\": [1, 1, 1, 2, 2, 1],\n",
        "            \"deps\": [\"nsubj\", \"ROOT\", \"dobj\", \"cc\", \"conj\", \"punct\"],\n",
        "        },\n",
        "    ),\n",
        "]\n",
        "\n",
        "\n",
        "def main(model=None, output_dir=None, n_iter=15):\n",
        "    \"\"\"Load the model, set up the pipeline and train the parser.\"\"\"\n",
        "    if model is not None:\n",
        "        nlp = spacy.load(model)  # load existing spaCy model\n",
        "        print(\"Loaded model '%s'\" % model)\n",
        "    else:\n",
        "        nlp = spacy.blank(\"en\")  # create blank Language class\n",
        "        print(\"Created blank 'en' model\")\n",
        "\n",
        "    # add the parser to the pipeline if it doesn't exist\n",
        "    # nlp.create_pipe works for built-ins that are registered with spaCy\n",
        "    if \"parser\" not in nlp.pipe_names:\n",
        "        parser = nlp.create_pipe(\"parser\")\n",
        "        nlp.add_pipe(parser, first=True)\n",
        "    # otherwise, get it, so we can add labels to it\n",
        "    else:\n",
        "        parser = nlp.get_pipe(\"parser\")\n",
        "\n",
        "    # add labels to the parser\n",
        "    for _, annotations in TRAIN_DATA:\n",
        "        for dep in annotations.get(\"deps\", []):\n",
        "            parser.add_label(dep)\n",
        "\n",
        "    # get names of other pipes to disable them during training\n",
        "    pipe_exceptions = [\"parser\", \"trf_wordpiecer\", \"trf_tok2vec\"]\n",
        "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]\n",
        "    with nlp.disable_pipes(*other_pipes):  # only train parser\n",
        "        optimizer = nlp.begin_training()\n",
        "        for itn in range(n_iter):\n",
        "            random.shuffle(TRAIN_DATA)\n",
        "            losses = {}\n",
        "            # batch up the examples using spaCy's minibatch\n",
        "            batches = minibatch(TRAIN_DATA, size=compounding(4.0, 32.0, 1.001))\n",
        "            for batch in batches:\n",
        "                texts, annotations = zip(*batch)\n",
        "                nlp.update(texts, annotations, sgd=optimizer, losses=losses)\n",
        "            print(\"Losses\", losses)\n",
        "\n",
        "    # test the trained model\n",
        "    test_text = \"I like securities.\"\n",
        "    doc = nlp(test_text)\n",
        "    print(\"Dependencies\", [(t.text, t.dep_, t.head.text) for t in doc])\n",
        "\n",
        "    # save model to output directory\n",
        "    if output_dir is not None:\n",
        "        output_dir = Path(output_dir)\n",
        "        if not output_dir.exists():\n",
        "            output_dir.mkdir()\n",
        "        nlp.to_disk(output_dir)\n",
        "        print(\"Saved model to\", output_dir)\n",
        "\n",
        "        # test the saved model\n",
        "        print(\"Loading from\", output_dir)\n",
        "        nlp2 = spacy.load(output_dir)\n",
        "        doc = nlp2(test_text)\n",
        "        print(\"Dependencies\", [(t.text, t.dep_, t.head.text) for t in doc])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fPQ7p4GQgbEo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "3271a8a9-5763-4df2-a9dd-da9b82405819"
      },
      "source": [
        "output_dir='/content/model'\n",
        "model=None\n",
        "main(model,output_dir)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Created blank 'en' model\n",
            "Losses {'parser': 7.215048972517252}\n",
            "Losses {'parser': 8.428646251559258}\n",
            "Losses {'parser': 7.299111798405647}\n",
            "Losses {'parser': 8.153000228106976}\n",
            "Losses {'parser': 7.851981949061155}\n",
            "Losses {'parser': 8.379417695105076}\n",
            "Losses {'parser': 7.761503949761391}\n",
            "Losses {'parser': 5.965851977467537}\n",
            "Losses {'parser': 4.8781580328941345}\n",
            "Losses {'parser': 4.568987902253866}\n",
            "Losses {'parser': 3.3852028623223305}\n",
            "Losses {'parser': 3.9229537528008223}\n",
            "Losses {'parser': 3.1050895154476166}\n",
            "Losses {'parser': 2.458554876036942}\n",
            "Losses {'parser': 3.0758475242182612}\n",
            "Dependencies [('I', 'nsubj', 'like'), ('like', 'ROOT', 'like'), ('securities', 'dobj', 'like'), ('.', 'dobj', 'like')]\n",
            "Saved model to /content/model\n",
            "Loading from /content/model\n",
            "Dependencies [('I', 'nsubj', 'like'), ('like', 'ROOT', 'like'), ('securities', 'dobj', 'like'), ('.', 'dobj', 'like')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0KPt5Lwlgj6Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Updating POS Tagger\n",
        "from __future__ import unicode_literals, print_function\n",
        "\n",
        "import plac\n",
        "import random\n",
        "from pathlib import Path\n",
        "import spacy\n",
        "from spacy.util import minibatch, compounding\n",
        "\n",
        "\n",
        "# You need to define a mapping from your data's part-of-speech tag names to the\n",
        "# Universal Part-of-Speech tag set, as spaCy includes an enum of these tags.\n",
        "# See here for the Universal Tag Set:\n",
        "# http://universaldependencies.github.io/docs/u/pos/index.html\n",
        "# You may also specify morphological features for your tags, from the universal\n",
        "# scheme.\n",
        "TAG_MAP = {\"N\": {\"pos\": \"NOUN\"}, \"V\": {\"pos\": \"VERB\"}, \"J\": {\"pos\": \"ADJ\"}}\n",
        "\n",
        "# Usually you'll read this in, of course. Data formats vary. Ensure your\n",
        "# strings are unicode and that the number of tags assigned matches spaCy's\n",
        "# tokenization. If not, you can always add a 'words' key to the annotations\n",
        "# that specifies the gold-standard tokenization, e.g.:\n",
        "# (\"Eatblueham\", {'words': ['Eat', 'blue', 'ham'], 'tags': ['V', 'J', 'N']})\n",
        "TRAIN_DATA = [\n",
        "    (\"I like green eggs\", {\"tags\": [\"N\", \"V\", \"J\", \"N\"]}),\n",
        "    (\"Eat blue ham\", {\"tags\": [\"V\", \"J\", \"N\"]}),\n",
        "]\n",
        "\n",
        "\n",
        "def main(lang=\"en\", output_dir=None, n_iter=25):\n",
        "    \"\"\"Create a new model, set up the pipeline and train the tagger. In order to\n",
        "    train the tagger with a custom tag map, we're creating a new Language\n",
        "    instance with a custom vocab.\n",
        "    \"\"\"\n",
        "    nlp = spacy.blank(lang)\n",
        "    # add the tagger to the pipeline\n",
        "    # nlp.create_pipe works for built-ins that are registered with spaCy\n",
        "    tagger = nlp.create_pipe(\"tagger\")\n",
        "    # Add the tags. This needs to be done before you start training.\n",
        "    for tag, values in TAG_MAP.items():\n",
        "        tagger.add_label(tag, values)\n",
        "    nlp.add_pipe(tagger)\n",
        "\n",
        "    optimizer = nlp.begin_training()\n",
        "    for i in range(n_iter):\n",
        "        random.shuffle(TRAIN_DATA)\n",
        "        losses = {}\n",
        "        # batch up the examples using spaCy's minibatch\n",
        "        batches = minibatch(TRAIN_DATA, size=compounding(4.0, 32.0, 1.001))\n",
        "        for batch in batches:\n",
        "            texts, annotations = zip(*batch)\n",
        "            nlp.update(texts, annotations, sgd=optimizer, losses=losses)\n",
        "        print(\"Losses\", losses)\n",
        "\n",
        "    # test the trained model\n",
        "    test_text = \"I like blue eggs\"\n",
        "    doc = nlp(test_text)\n",
        "    print(\"Tags\", [(t.text, t.tag_, t.pos_) for t in doc])\n",
        "\n",
        "    # save model to output directory\n",
        "    if output_dir is not None:\n",
        "        output_dir = Path(output_dir)\n",
        "        if not output_dir.exists():\n",
        "            output_dir.mkdir()\n",
        "        nlp.to_disk(output_dir)\n",
        "        print(\"Saved model to\", output_dir)\n",
        "\n",
        "        # test the save model\n",
        "        print(\"Loading from\", output_dir)\n",
        "        nlp2 = spacy.load(output_dir)\n",
        "        doc = nlp2(test_text)\n",
        "        print(\"Tags\", [(t.text, t.tag_, t.pos_) for t in doc])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MzU9mhkjhLHh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 513
        },
        "outputId": "93949d51-26a2-4774-f0d3-027dfbf8128a"
      },
      "source": [
        "output_dir='/content/model1'\n",
        "main()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.6/runpy.py:193: UserWarning: [W022] Training a new part-of-speech tagger using a model with no lemmatization rules or data. This means that the trained model may not be able to lemmatize correctly. If this is intentional or the language you're using doesn't have lemmatization data, you can ignore this warning by setting SPACY_WARNING_IGNORE=W022. If this is surprising, make sure you have the spacy-lookups-data package installed.\n",
            "  \"__main__\", mod_spec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Losses {'tagger': 6.883332252502441}\n",
            "Losses {'tagger': 6.839642524719238}\n",
            "Losses {'tagger': 6.701166152954102}\n",
            "Losses {'tagger': 6.430505752563477}\n",
            "Losses {'tagger': 5.922542572021484}\n",
            "Losses {'tagger': 5.04991340637207}\n",
            "Losses {'tagger': 3.8205037117004395}\n",
            "Losses {'tagger': 2.515399932861328}\n",
            "Losses {'tagger': 1.435142993927002}\n",
            "Losses {'tagger': 0.6975435018539429}\n",
            "Losses {'tagger': 0.2880050539970398}\n",
            "Losses {'tagger': 0.10575447976589203}\n",
            "Losses {'tagger': 0.036467667669057846}\n",
            "Losses {'tagger': 0.012316400185227394}\n",
            "Losses {'tagger': 0.004149605054408312}\n",
            "Losses {'tagger': 0.0014039292000234127}\n",
            "Losses {'tagger': 0.00047847648966126144}\n",
            "Losses {'tagger': 0.00016427092486992478}\n",
            "Losses {'tagger': 5.7249595556641e-05}\n",
            "Losses {'tagger': 2.0773106371052563e-05}\n",
            "Losses {'tagger': 7.974735126481391e-06}\n",
            "Losses {'tagger': 3.2602479222987313e-06}\n",
            "Losses {'tagger': 1.4231400200515054e-06}\n",
            "Losses {'tagger': 6.614289986828226e-07}\n",
            "Losses {'tagger': 3.271207447141933e-07}\n",
            "Tags [('I', 'N', 'NOUN'), ('like', 'V', 'VERB'), ('blue', 'J', 'ADJ'), ('eggs', 'N', 'NOUN')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5FjWGTdhx4I",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "572f05e8-a849-4355-92c2-cd235587be15"
      },
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(\"find a hotel with good wifi\")\n",
        "print([(t.text, t.dep_, t.head.text) for t in doc if t.dep_ != '-'])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('find', 'ROOT', 'find'), ('a', 'det', 'hotel'), ('hotel', 'dobj', 'find'), ('with', 'prep', 'hotel'), ('good', 'amod', 'wifi'), ('wifi', 'pobj', 'with')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F91IHrbSiufV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Training intent parser\n",
        "\n",
        "from __future__ import unicode_literals, print_function\n",
        "\n",
        "import plac\n",
        "import random\n",
        "from pathlib import Path\n",
        "import spacy\n",
        "from spacy.util import minibatch, compounding\n",
        "\n",
        "\n",
        "# training data: texts, heads and dependency labels\n",
        "# for no relation, we simply chose an arbitrary dependency label, e.g. '-'\n",
        "TRAIN_DATA = [\n",
        "    (\n",
        "        \"find a cafe with great wifi\",\n",
        "        {\n",
        "            \"heads\": [0, 2, 0, 5, 5, 2],  # index of token head\n",
        "            \"deps\": [\"ROOT\", \"-\", \"PLACE\", \"-\", \"QUALITY\", \"ATTRIBUTE\"],\n",
        "        },\n",
        "    ),\n",
        "    (\n",
        "        \"find a hotel near the beach\",\n",
        "        {\n",
        "            \"heads\": [0, 2, 0, 5, 5, 2],\n",
        "            \"deps\": [\"ROOT\", \"-\", \"PLACE\", \"QUALITY\", \"-\", \"ATTRIBUTE\"],\n",
        "        },\n",
        "    ),\n",
        "    (\n",
        "        \"find me the closest gym that's open late\",\n",
        "        {\n",
        "            \"heads\": [0, 0, 4, 4, 0, 6, 4, 6, 6],\n",
        "            \"deps\": [\n",
        "                \"ROOT\",\n",
        "                \"-\",\n",
        "                \"-\",\n",
        "                \"QUALITY\",\n",
        "                \"PLACE\",\n",
        "                \"-\",\n",
        "                \"-\",\n",
        "                \"ATTRIBUTE\",\n",
        "                \"TIME\",\n",
        "            ],\n",
        "        },\n",
        "    ),\n",
        "    (\n",
        "        \"show me the cheapest store that sells flowers\",\n",
        "        {\n",
        "            \"heads\": [0, 0, 4, 4, 0, 4, 4, 4],  # attach \"flowers\" to store!\n",
        "            \"deps\": [\"ROOT\", \"-\", \"-\", \"QUALITY\", \"PLACE\", \"-\", \"-\", \"PRODUCT\"],\n",
        "        },\n",
        "    ),\n",
        "    (\n",
        "        \"find a nice restaurant in london\",\n",
        "        {\n",
        "            \"heads\": [0, 3, 3, 0, 3, 3],\n",
        "            \"deps\": [\"ROOT\", \"-\", \"QUALITY\", \"PLACE\", \"-\", \"LOCATION\"],\n",
        "        },\n",
        "    ),\n",
        "    (\n",
        "        \"show me the coolest hostel in berlin\",\n",
        "        {\n",
        "            \"heads\": [0, 0, 4, 4, 0, 4, 4],\n",
        "            \"deps\": [\"ROOT\", \"-\", \"-\", \"QUALITY\", \"PLACE\", \"-\", \"LOCATION\"],\n",
        "        },\n",
        "    ),\n",
        "    (\n",
        "        \"find a good italian restaurant near work\",\n",
        "        {\n",
        "            \"heads\": [0, 4, 4, 4, 0, 4, 5],\n",
        "            \"deps\": [\n",
        "                \"ROOT\",\n",
        "                \"-\",\n",
        "                \"QUALITY\",\n",
        "                \"ATTRIBUTE\",\n",
        "                \"PLACE\",\n",
        "                \"ATTRIBUTE\",\n",
        "                \"LOCATION\",\n",
        "            ],\n",
        "        },\n",
        "    ),\n",
        "]\n",
        "\n",
        "\n",
        "def main(model=None, output_dir=None, n_iter=15):\n",
        "    if model is not None:\n",
        "        nlp = spacy.load(model)  # load existing spaCy model\n",
        "        print(\"Loaded model '%s'\" % model)\n",
        "    else:\n",
        "        nlp = spacy.blank(\"en\")  # create blank Language class\n",
        "        print(\"Created blank 'en' model\")\n",
        "\n",
        "    # We'll use the built-in dependency parser class, but we want to create a\n",
        "    # fresh instance â€“ just in case.\n",
        "    if \"parser\" in nlp.pipe_names:\n",
        "        nlp.remove_pipe(\"parser\")\n",
        "    parser = nlp.create_pipe(\"parser\")\n",
        "    nlp.add_pipe(parser, first=True)\n",
        "\n",
        "    for text, annotations in TRAIN_DATA:\n",
        "        for dep in annotations.get(\"deps\", []):\n",
        "            parser.add_label(dep)\n",
        "\n",
        "    pipe_exceptions = [\"parser\", \"trf_wordpiecer\", \"trf_tok2vec\"]\n",
        "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]\n",
        "    with nlp.disable_pipes(*other_pipes):  # only train parser\n",
        "        optimizer = nlp.begin_training()\n",
        "        for itn in range(n_iter):\n",
        "            random.shuffle(TRAIN_DATA)\n",
        "            losses = {}\n",
        "            # batch up the examples using spaCy's minibatch\n",
        "            batches = minibatch(TRAIN_DATA, size=compounding(4.0, 32.0, 1.001))\n",
        "            for batch in batches:\n",
        "                texts, annotations = zip(*batch)\n",
        "                nlp.update(texts, annotations, sgd=optimizer, losses=losses)\n",
        "            print(\"Losses\", losses)\n",
        "\n",
        "    # test the trained model\n",
        "    test_model(nlp)\n",
        "\n",
        "    # save model to output directory\n",
        "    if output_dir is not None:\n",
        "        output_dir = Path(output_dir)\n",
        "        if not output_dir.exists():\n",
        "            output_dir.mkdir()\n",
        "        nlp.to_disk(output_dir)\n",
        "        print(\"Saved model to\", output_dir)\n",
        "\n",
        "        # test the saved model\n",
        "        print(\"Loading from\", output_dir)\n",
        "        nlp2 = spacy.load(output_dir)\n",
        "        test_model(nlp2)\n",
        "\n",
        "\n",
        "def test_model(nlp):\n",
        "    texts = [\n",
        "        \"find a hotel with good wifi\",\n",
        "        \"find me the cheapest gym near work\",\n",
        "        \"show me the best hotel in berlin\",\n",
        "    ]\n",
        "    docs = nlp.pipe(texts)\n",
        "    for doc in docs:\n",
        "        print(doc.text)\n",
        "        print([(t.text, t.dep_, t.head.text) for t in doc if t.dep_ != \"-\"])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sTVYfa8qjotz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "8391d040-6761-4877-cdef-8ef3db60ddd7"
      },
      "source": [
        "main()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Created blank 'en' model\n",
            "Losses {'parser': 26.135916590690613}\n",
            "Losses {'parser': 31.599409893155098}\n",
            "Losses {'parser': 33.30124479532242}\n",
            "Losses {'parser': 31.203646689653397}\n",
            "Losses {'parser': 32.94919466972351}\n",
            "Losses {'parser': 30.513385832309723}\n",
            "Losses {'parser': 26.337806463241577}\n",
            "Losses {'parser': 28.296614915132523}\n",
            "Losses {'parser': 24.470583319664}\n",
            "Losses {'parser': 17.566462516784668}\n",
            "Losses {'parser': 12.168225288391113}\n",
            "Losses {'parser': 8.55454621091485}\n",
            "Losses {'parser': 4.41173727158457}\n",
            "Losses {'parser': 2.0656678290106356}\n",
            "Losses {'parser': 0.7076657166689984}\n",
            "find a hotel with good wifi\n",
            "[('find', 'ROOT', 'find'), ('hotel', 'PLACE', 'find'), ('good', 'QUALITY', 'wifi'), ('wifi', 'ATTRIBUTE', 'hotel')]\n",
            "find me the cheapest gym near work\n",
            "[('find', 'ROOT', 'find'), ('cheapest', 'QUALITY', 'gym'), ('gym', 'PLACE', 'find'), ('near', 'ATTRIBUTE', 'gym'), ('work', 'LOCATION', 'near')]\n",
            "show me the best hotel in berlin\n",
            "[('show', 'ROOT', 'show'), ('best', 'QUALITY', 'hotel'), ('hotel', 'PLACE', 'show'), ('berlin', 'LOCATION', 'hotel')]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}