{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Spacy_Training_Model_1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "HRu78qvN4sls",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data = [\n",
        "    (\"Uber blew through $1 million a week\", [(0, 4, 'ORG')]),\n",
        "    (\"Android Pay expands to Canada\", [(0, 11, 'PRODUCT'), (23, 30, 'GPE')]),\n",
        "    (\"Spotify steps up Asia expansion\", [(0, 8, \"ORG\"), (17, 21, \"LOC\")]),\n",
        "    (\"Google Maps launches location sharing\", [(0, 11, \"PRODUCT\")]),\n",
        "    (\"Google rebrands its business apps\", [(0, 6, \"ORG\")]),\n",
        "    (\"look what i found on google! ðŸ˜‚\", [(21, 27, \"PRODUCT\")])]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YRg0jAu6500-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import spacy\n",
        "import random\n",
        "#Simple Training Loop\n",
        "TRAIN_DATA = [\n",
        "        (\"Uber blew through $1 million a week\", {\"entities\": [(0, 4, \"ORG\")]}),\n",
        "        (\"Google rebrands its business apps\", {\"entities\": [(0, 6, \"ORG\")]})]\n",
        "\n",
        "nlp = spacy.blank(\"en\")\n",
        "optimizer = nlp.begin_training()\n",
        "for i in range(20):\n",
        "    random.shuffle(TRAIN_DATA)\n",
        "    for text, annotations in TRAIN_DATA:\n",
        "        nlp.update([text], [annotations], sgd=optimizer)\n",
        "nlp.to_disk(\"/model\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6R2GKcOq8puU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Updating the Named Entity Recognizer\n",
        "from __future__ import unicode_literals, print_function\n",
        "\n",
        "import plac\n",
        "import random\n",
        "from pathlib import Path\n",
        "import spacy\n",
        "from spacy.util import minibatch, compounding\n",
        "\n",
        "\n",
        "# training data\n",
        "TRAIN_DATA = [\n",
        "    (\"Who is Shaka Khan?\", {\"entities\": [(7, 17, \"PERSON\")]}),\n",
        "    (\"I like London and Berlin.\", {\"entities\": [(7, 13, \"LOC\"), (18, 24, \"LOC\")]}),\n",
        "]\n",
        "\n",
        "\n",
        "def main(model=None, output_dir=None, n_iter=100):\n",
        "    \"\"\"Load the model, set up the pipeline and train the entity recognizer.\"\"\"\n",
        "    if model is not None:\n",
        "        nlp = spacy.load(model)  # load existing spaCy model\n",
        "        print(\"Loaded model '%s'\" % model)\n",
        "    else:\n",
        "        nlp = spacy.blank(\"en\")  # create blank Language class\n",
        "        print(\"Created blank 'en' model\")\n",
        "\n",
        "    # create the built-in pipeline components and add them to the pipeline\n",
        "    # nlp.create_pipe works for built-ins that are registered with spaCy\n",
        "    if \"ner\" not in nlp.pipe_names:\n",
        "        ner = nlp.create_pipe(\"ner\")\n",
        "        nlp.add_pipe(ner, last=True)\n",
        "    # otherwise, get it so we can add labels\n",
        "    else:\n",
        "        ner = nlp.get_pipe(\"ner\")\n",
        "\n",
        "    # add labels\n",
        "    for _, annotations in TRAIN_DATA:\n",
        "        for ent in annotations.get(\"entities\"):\n",
        "            ner.add_label(ent[2])\n",
        "\n",
        "    # get names of other pipes to disable them during training\n",
        "    pipe_exceptions = [\"ner\", \"trf_wordpiecer\", \"trf_tok2vec\"]\n",
        "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]\n",
        "    with nlp.disable_pipes(*other_pipes):  # only train NER\n",
        "        # reset and initialize the weights randomly â€“ but only if we're\n",
        "        # training a new model\n",
        "        if model is None:\n",
        "            nlp.begin_training()\n",
        "        for itn in range(n_iter):\n",
        "            random.shuffle(TRAIN_DATA)\n",
        "            losses = {}\n",
        "            # batch up the examples using spaCy's minibatch\n",
        "            batches = minibatch(TRAIN_DATA, size=compounding(4.0, 32.0, 1.001))\n",
        "            for batch in batches:\n",
        "                texts, annotations = zip(*batch)\n",
        "                nlp.update(\n",
        "                    texts,  # batch of texts\n",
        "                    annotations,  # batch of annotations\n",
        "                    drop=0.5,  # dropout - make it harder to memorise data\n",
        "                    losses=losses,\n",
        "                )\n",
        "            print(\"Losses\", losses)\n",
        "\n",
        "    # test the trained model\n",
        "    for text, _ in TRAIN_DATA:\n",
        "        doc = nlp(text)\n",
        "        print(\"Entities\", [(ent.text, ent.label_) for ent in doc.ents])\n",
        "        print(\"Tokens\", [(t.text, t.ent_type_, t.ent_iob) for t in doc])\n",
        "\n",
        "    # save model to output directory\n",
        "    if output_dir is not None:\n",
        "        output_dir = Path(output_dir)\n",
        "        if not output_dir.exists():\n",
        "            output_dir.mkdir()\n",
        "        nlp.to_disk(output_dir)\n",
        "        print(\"Saved model to\", output_dir)\n",
        "\n",
        "        # test the saved model\n",
        "        print(\"Loading from\", output_dir)\n",
        "        nlp2 = spacy.load(output_dir)\n",
        "        for text, _ in TRAIN_DATA:\n",
        "            doc = nlp2(text)\n",
        "            print(\"Entities\", [(ent.text, ent.label_) for ent in doc.ents])\n",
        "            print(\"Tokens\", [(t.text, t.ent_type_, t.ent_iob) for t in doc])\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EIJwXggu9kc-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7701cac3-cff4-49e7-995e-0b9947d694b7"
      },
      "source": [
        "output_dir='/content/model'\n",
        "model=None\n",
        "main(model,output_dir)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Created blank 'en' model\n",
            "Losses {'ner': 9.899998903274536}\n",
            "Losses {'ner': 8.778671145439148}\n",
            "Losses {'ner': 9.327803254127502}\n",
            "Losses {'ner': 9.057472348213196}\n",
            "Losses {'ner': 8.634560227394104}\n",
            "Losses {'ner': 8.618420839309692}\n",
            "Losses {'ner': 8.21012806892395}\n",
            "Losses {'ner': 7.534020781517029}\n",
            "Losses {'ner': 6.9119760394096375}\n",
            "Losses {'ner': 6.430559277534485}\n",
            "Losses {'ner': 6.145543932914734}\n",
            "Losses {'ner': 6.0814735889434814}\n",
            "Losses {'ner': 5.569485306739807}\n",
            "Losses {'ner': 4.92256361246109}\n",
            "Losses {'ner': 4.9597503542900085}\n",
            "Losses {'ner': 4.940003156661987}\n",
            "Losses {'ner': 4.626280389726162}\n",
            "Losses {'ner': 4.222173266112804}\n",
            "Losses {'ner': 4.707662243396044}\n",
            "Losses {'ner': 4.760649351403117}\n",
            "Losses {'ner': 4.815691292285919}\n",
            "Losses {'ner': 3.6351468916982412}\n",
            "Losses {'ner': 4.311138439923525}\n",
            "Losses {'ner': 3.8294673934578896}\n",
            "Losses {'ner': 4.072155494010076}\n",
            "Losses {'ner': 4.0062944907695055}\n",
            "Losses {'ner': 4.028544634580612}\n",
            "Losses {'ner': 2.7827253192663193}\n",
            "Losses {'ner': 2.9915042370557785}\n",
            "Losses {'ner': 2.667089257389307}\n",
            "Losses {'ner': 2.2165964543819427}\n",
            "Losses {'ner': 2.2154704332351685}\n",
            "Losses {'ner': 2.441381487995386}\n",
            "Losses {'ner': 2.1006516814231873}\n",
            "Losses {'ner': 1.518285684287548}\n",
            "Losses {'ner': 3.329235851764679}\n",
            "Losses {'ner': 3.8699178397655487}\n",
            "Losses {'ner': 1.600837379693985}\n",
            "Losses {'ner': 1.6207017675042152}\n",
            "Losses {'ner': 2.006413203664124}\n",
            "Losses {'ner': 2.7244647592306137}\n",
            "Losses {'ner': 1.3265271140262485}\n",
            "Losses {'ner': 2.6251699288841337}\n",
            "Losses {'ner': 2.4957343998830765}\n",
            "Losses {'ner': 3.0435118324821815}\n",
            "Losses {'ner': 2.2015882150735706}\n",
            "Losses {'ner': 2.340925179858459}\n",
            "Losses {'ner': 2.4479951662942767}\n",
            "Losses {'ner': 2.0623688351770397}\n",
            "Losses {'ner': 1.4672698966460302}\n",
            "Losses {'ner': 1.8310245296452194}\n",
            "Losses {'ner': 1.1634221197164152}\n",
            "Losses {'ner': 1.6147955376654863}\n",
            "Losses {'ner': 1.0453911467357102}\n",
            "Losses {'ner': 1.5548407143214718}\n",
            "Losses {'ner': 1.1009695763495984}\n",
            "Losses {'ner': 1.9623616950120777}\n",
            "Losses {'ner': 0.7278376783779095}\n",
            "Losses {'ner': 0.9632642057549674}\n",
            "Losses {'ner': 0.7161518031207379}\n",
            "Losses {'ner': 0.2504973131872248}\n",
            "Losses {'ner': 0.43338960432447493}\n",
            "Losses {'ner': 0.7750905430875719}\n",
            "Losses {'ner': 0.5022396670383387}\n",
            "Losses {'ner': 0.6711409516283311}\n",
            "Losses {'ner': 0.23759456706898163}\n",
            "Losses {'ner': 0.4017716258886139}\n",
            "Losses {'ner': 0.2402628153504338}\n",
            "Losses {'ner': 0.14764418734876017}\n",
            "Losses {'ner': 0.118209484131512}\n",
            "Losses {'ner': 0.10891659526021158}\n",
            "Losses {'ner': 0.07266993533312416}\n",
            "Losses {'ner': 0.004925473838966354}\n",
            "Losses {'ner': 0.013563909335033486}\n",
            "Losses {'ner': 0.0005612315944745205}\n",
            "Losses {'ner': 0.012013221258087015}\n",
            "Losses {'ner': 0.0009513108145142724}\n",
            "Losses {'ner': 0.002456424522485179}\n",
            "Losses {'ner': 0.0015380203571950801}\n",
            "Losses {'ner': 0.014847139773337403}\n",
            "Losses {'ner': 0.005627613251419916}\n",
            "Losses {'ner': 0.11587136481178106}\n",
            "Losses {'ner': 5.605334045921673e-05}\n",
            "Losses {'ner': 0.00018933707210599948}\n",
            "Losses {'ner': 0.011546445424582719}\n",
            "Losses {'ner': 0.05177681442510451}\n",
            "Losses {'ner': 0.00014594948996204948}\n",
            "Losses {'ner': 0.00011936420887441804}\n",
            "Losses {'ner': 4.856407010195296e-05}\n",
            "Losses {'ner': 2.0376872371663524e-06}\n",
            "Losses {'ner': 3.867942752422415e-06}\n",
            "Losses {'ner': 0.0017706574373991812}\n",
            "Losses {'ner': 7.774037085184204e-07}\n",
            "Losses {'ner': 7.841043540781652e-08}\n",
            "Losses {'ner': 2.3520730096038278e-05}\n",
            "Losses {'ner': 8.250268798389374e-08}\n",
            "Losses {'ner': 2.426283401362e-06}\n",
            "Losses {'ner': 0.00012840595542977506}\n",
            "Losses {'ner': 2.925385583176572e-06}\n",
            "Losses {'ner': 1.035381493666443e-07}\n",
            "Entities [('London', 'LOC'), ('Berlin', 'LOC')]\n",
            "Tokens [('I', '', 2), ('like', '', 2), ('London', 'LOC', 3), ('and', '', 2), ('Berlin', 'LOC', 3), ('.', '', 2)]\n",
            "Entities [('Shaka Khan', 'PERSON')]\n",
            "Tokens [('Who', '', 2), ('is', '', 2), ('Shaka', 'PERSON', 3), ('Khan', 'PERSON', 1), ('?', '', 2)]\n",
            "Saved model to /content/model\n",
            "Loading from /content/model\n",
            "Entities [('London', 'LOC'), ('Berlin', 'LOC')]\n",
            "Tokens [('I', '', 2), ('like', '', 2), ('London', 'LOC', 3), ('and', '', 2), ('Berlin', 'LOC', 3), ('.', '', 2)]\n",
            "Entities [('Shaka Khan', 'PERSON')]\n",
            "Tokens [('Who', '', 2), ('is', '', 2), ('Shaka', 'PERSON', 3), ('Khan', 'PERSON', 1), ('?', '', 2)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T1ak9Ip8-KCY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Training additional entity type\n",
        "from __future__ import unicode_literals, print_function\n",
        "\n",
        "import plac\n",
        "import random\n",
        "from pathlib import Path\n",
        "import spacy\n",
        "from spacy.util import minibatch, compounding\n",
        "\n",
        "\n",
        "# new entity label\n",
        "LABEL = \"ANIMAL\"\n",
        "\n",
        "# training data\n",
        "# Note: If you're using an existing model, make sure to mix in examples of\n",
        "# other entity types that spaCy correctly recognized before. Otherwise, your\n",
        "# model might learn the new type, but \"forget\" what it previously knew.\n",
        "# https://explosion.ai/blog/pseudo-rehearsal-catastrophic-forgetting\n",
        "TRAIN_DATA = [\n",
        "    (\n",
        "        \"Horses are too tall and they pretend to care about your feelings\",\n",
        "        {\"entities\": [(0, 6, LABEL)]},\n",
        "    ),\n",
        "    (\"Do they bite?\", {\"entities\": []}),\n",
        "    (\n",
        "        \"horses are too tall and they pretend to care about your feelings\",\n",
        "        {\"entities\": [(0, 6, LABEL)]},\n",
        "    ),\n",
        "    (\"horses pretend to care about your feelings\", {\"entities\": [(0, 6, LABEL)]}),\n",
        "    (\n",
        "        \"they pretend to care about your feelings, those horses\",\n",
        "        {\"entities\": [(48, 54, LABEL)]},\n",
        "    ),\n",
        "    (\"horses?\", {\"entities\": [(0, 6, LABEL)]}),\n",
        "]\n",
        "\n",
        "\n",
        "def main(model=None, new_model_name=\"animal\", output_dir=None, n_iter=30):\n",
        "    \"\"\"Set up the pipeline and entity recognizer, and train the new entity.\"\"\"\n",
        "    random.seed(0)\n",
        "    if model is not None:\n",
        "        nlp = spacy.load(model)  # load existing spaCy model\n",
        "        print(\"Loaded model '%s'\" % model)\n",
        "    else:\n",
        "        nlp = spacy.blank(\"en\")  # create blank Language class\n",
        "        print(\"Created blank 'en' model\")\n",
        "    # Add entity recognizer to model if it's not in the pipeline\n",
        "    # nlp.create_pipe works for built-ins that are registered with spaCy\n",
        "    if \"ner\" not in nlp.pipe_names:\n",
        "        ner = nlp.create_pipe(\"ner\")\n",
        "        nlp.add_pipe(ner)\n",
        "    # otherwise, get it, so we can add labels to it\n",
        "    else:\n",
        "        ner = nlp.get_pipe(\"ner\")\n",
        "\n",
        "    ner.add_label(LABEL)  # add new entity label to entity recognizer\n",
        "    # Adding extraneous labels shouldn't mess anything up\n",
        "    ner.add_label(\"VEGETABLE\")\n",
        "    if model is None:\n",
        "        optimizer = nlp.begin_training()\n",
        "    else:\n",
        "        optimizer = nlp.resume_training()\n",
        "    move_names = list(ner.move_names)\n",
        "    # get names of other pipes to disable them during training\n",
        "    pipe_exceptions = [\"ner\", \"trf_wordpiecer\", \"trf_tok2vec\"]\n",
        "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]\n",
        "    with nlp.disable_pipes(*other_pipes):  # only train NER\n",
        "        sizes = compounding(1.0, 4.0, 1.001)\n",
        "        # batch up the examples using spaCy's minibatch\n",
        "        for itn in range(n_iter):\n",
        "            random.shuffle(TRAIN_DATA)\n",
        "            batches = minibatch(TRAIN_DATA, size=sizes)\n",
        "            losses = {}\n",
        "            for batch in batches:\n",
        "                texts, annotations = zip(*batch)\n",
        "                nlp.update(texts, annotations, sgd=optimizer, drop=0.35, losses=losses)\n",
        "            print(\"Losses\", losses)\n",
        "\n",
        "    # test the trained model\n",
        "    test_text = \"Do you like horses?\"\n",
        "    doc = nlp(test_text)\n",
        "    print(\"Entities in '%s'\" % test_text)\n",
        "    for ent in doc.ents:\n",
        "        print(ent.label_, ent.text)\n",
        "\n",
        "    # save model to output directory\n",
        "    if output_dir is not None:\n",
        "        output_dir = Path(output_dir)\n",
        "        if not output_dir.exists():\n",
        "            output_dir.mkdir()\n",
        "        nlp.meta[\"name\"] = new_model_name  # rename model\n",
        "        nlp.to_disk(output_dir)\n",
        "        print(\"Saved model to\", output_dir)\n",
        "\n",
        "        # test the saved model\n",
        "        print(\"Loading from\", output_dir)\n",
        "        nlp2 = spacy.load(output_dir)\n",
        "        # Check the classes have loaded back consistently\n",
        "        assert nlp2.get_pipe(\"ner\").move_names == move_names\n",
        "        doc2 = nlp2(test_text)\n",
        "        for ent in doc2.ents:\n",
        "            print(ent.label_, ent.text)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KPJSczLF_JwU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 578
        },
        "outputId": "194b2d80-627e-4db0-de35-b92c975284f5"
      },
      "source": [
        "output_dir='/content/model1'\n",
        "model=None\n",
        "main(model,output_dir)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Created blank 'en' model\n",
            "Losses {'ner': 39.923790991306305}\n",
            "Losses {'ner': 23.07769750058651}\n",
            "Losses {'ner': 8.262818779381632}\n",
            "Losses {'ner': 9.4909405179273}\n",
            "Losses {'ner': 7.567021398682527}\n",
            "Losses {'ner': 3.465229131950764}\n",
            "Losses {'ner': 2.665531281415497}\n",
            "Losses {'ner': 0.9080539423931198}\n",
            "Losses {'ner': 0.1693589918164926}\n",
            "Losses {'ner': 0.046554384368393355}\n",
            "Losses {'ner': 0.11450711571484004}\n",
            "Losses {'ner': 9.981102381958824e-05}\n",
            "Losses {'ner': 0.5722596423186259}\n",
            "Losses {'ner': 8.588633210141595e-08}\n",
            "Losses {'ner': 0.0027663579868935674}\n",
            "Losses {'ner': 1.5806595525633335e-07}\n",
            "Losses {'ner': 4.881471831645865e-07}\n",
            "Losses {'ner': 2.1805172597136997e-05}\n",
            "Losses {'ner': 1.1295632672327314e-06}\n",
            "Losses {'ner': 4.6953457011313805e-05}\n",
            "Losses {'ner': 6.146585180770046e-08}\n",
            "Losses {'ner': 1.389072286270877e-08}\n",
            "Losses {'ner': 8.947088693782281e-08}\n",
            "Losses {'ner': 1.460301303956955e-07}\n",
            "Losses {'ner': 2.15435254919512e-08}\n",
            "Losses {'ner': 0.025911011836624188}\n",
            "Losses {'ner': 1.478962169014119e-07}\n",
            "Losses {'ner': 1.7268744995498656e-05}\n",
            "Losses {'ner': 0.00017670372084590513}\n",
            "Losses {'ner': 2.837623352953482e-06}\n",
            "Entities in 'Do you like horses?'\n",
            "ANIMAL horses\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}